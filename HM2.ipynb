{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiWNL3jsZagn65L1DkL7qa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LazarusN97/CovidRecovery/blob/main/HM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDeHj8QXN-v6"
      },
      "outputs": [],
      "source": [
        "#====================================================================================#\n",
        "# PURPOSE: Simple intoduction in pandas & numpy - HW2\n",
        "#\n",
        "# Date:   May 2024\n",
        "# Author: Lazaros Nikolaos\n",
        "#====================================================================================#\n",
        "#import the forementioned libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Laod the data but only 10000 first rows from each csv file\n",
        "df1 = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/public_up_to_150k_1_230930.csv',nrows=1000000)\n",
        "print(df1.head())\n",
        "df2 = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/public_up_to_150k_2_230930.csv',nrows=1000000)\n",
        "df3 = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/public_up_to_150k_3_230930.csv',nrows=1000000)\n",
        "df4 = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/public_up_to_150k_4_230930.csv',nrows=1000000)\n",
        "df5 = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/public_up_to_150k_5_230930.csv',nrows=1000000)\n",
        "df6 = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/public_up_to_150k_6_230930.csv',nrows=1000000)\n",
        "df7 = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/public_up_to_150k_7_230930.csv',nrows=1000000)\n",
        "df8 = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/public_up_to_150k_8_230930.csv',nrows=1000000)\n",
        "df9 = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/public_up_to_150k_9_230930.csv',nrows=1000000)\n",
        "df10 = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/public_up_to_150k_10_230930.csv',nrows=1000000)\n",
        "df11 = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/public_up_to_150k_11_230930.csv',nrows=1000000)\n",
        "df12 = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/public_up_to_150k_12_230930.csv',nrows=1000000)\n",
        "\n",
        "jobCity = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/Job Postings - City - Weekly.csv')\n",
        "jobCounty = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/Job Postings - County - Weekly.csv')\n",
        "jobNational = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/Job Postings - National - Weekly.csv')\n",
        "dfState = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/Job Postings - State - Weekly.csv')\n",
        "dfIndustry = pd.read_csv('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/Job Postings Industry Shares - National - 2020.csv')\n",
        "\n",
        "pop = pd.read_excel('/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/RawData/co-est2023-pop.xlsx')\n",
        "pop.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Concatenate DataFrames by rows\n",
        "df_concat = pd.concat([df1, df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12], axis=0, ignore_index=True)\n",
        "print(df_concat)\n",
        "\n",
        "##lets convert the data types to save more memory\n",
        "# Function to convert data types\n",
        "def convert_dtypes(df):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == np.float64:\n",
        "            df[col] = df[col].astype(np.float32)\n",
        "        elif df[col].dtype == np.int64:\n",
        "            df[col] = df[col].astype(np.int32)\n",
        "        # Optionally handle string conversion if needed\n",
        "        # elif df[col].dtype == object:\n",
        "        #     try:\n",
        "        #         df[col] = df[col].astype(np.str_)\n",
        "        #     except:\n",
        "        #         pass\n",
        "    return df\n",
        "\n",
        "df_concat = convert_dtypes(df_concat)\n",
        "df_concat.info()\n",
        "\n",
        "#lets see our variables\n",
        "print(df_concat.columns)\n",
        "#basic summary of our df\n",
        "# Get a summary of the DataFrame\n",
        "df_concat.info()\n",
        "df_concat.head()\n",
        "\n",
        "#Clean the data\n",
        "#lets check for NAs\n",
        "#number of na\n",
        "print(df_concat.isna().sum())\n",
        "\n",
        "#we want to work on columns that have to do with loan risk assesment and the borrowers capability of paying the loan back.\n",
        "#So for loan risk assessment we focus on columns that provide insight into the financial health and stability of the borrower.\n",
        "#some of these columns are subsetted in the data frame bellow\n",
        "\n",
        "columns_to_keep = [\n",
        "    'DateApproved',\n",
        "    'CurrentApprovalAmount',\n",
        "    'LoanNumber',\n",
        "    'BorrowerCity',\n",
        "    'BorrowerState',\n",
        "    'BorrowerZip',\n",
        "    'LoanStatus',\n",
        "    'Term',\n",
        "    'SBAGuarantyPercentage',\n",
        "    'InitialApprovalAmount',\n",
        "    'UndisbursedAmount',\n",
        "    'BusinessAgeDescription',\n",
        "    'JobsReported',\n",
        "    'NAICSCode',\n",
        "    'Race',\n",
        "    'Ethnicity',\n",
        "    'Gender',\n",
        "    'Veteran',\n",
        "    'BusinessType',\n",
        "    'ForgivenessAmount',\n",
        "    'ForgivenessDate'\n",
        "]\n",
        "\n",
        "df_final = df_concat[columns_to_keep]\n",
        "len(df_final)\n",
        "df_final.info()\n",
        "print(df_final.isna().sum())\n",
        "\n",
        "#lets drop the rest of the NA left\n",
        "df_final= df_final.dropna()\n",
        "df_final.info()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##EDA\n",
        "##import packages for EDA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#simple describe command to see some metrics like the mean, median, min and max calue for each variable.\n",
        "df_final.describe()\n",
        "\n",
        "\n",
        "##some histograms to gain insight on our data\n",
        "df_final.hist(figsize=(14, 10), bins=30, edgecolor='black')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#lets merge the data based on geography and time\n",
        "df_final.head()\n",
        "pop.head()\n",
        "\n",
        "\n",
        "##lets calculate the loan pef 100k\n",
        "\n",
        "\n",
        "# Analyze Data\n",
        "# Merge PPP and population data by state\n",
        "df_final['BorrowerState'] = df_final['BorrowerState'].str.upper()\n",
        "state_loan_sum = df_final.groupby('BorrowerState')['CurrentApprovalAmount'].sum().reset_index()\n",
        "\n",
        "pop['geographic area'] = pop['geographic area'].astype(str).str.zfill(2)\n",
        "pop['geographic area'] = pop['geographic area'].str.split(',').str[-1].str.strip()\n",
        "state_population = pop.groupby('geographic area').sum().reset_index()\n",
        "\n",
        "# State abbreviation to full name mapping\n",
        "state_mapping = {\n",
        "    'CA': 'California', 'NY': 'New York', 'TX': 'Texas', 'FL': 'Florida', 'WA': 'Washington',\n",
        "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CO': 'Colorado',\n",
        "    'CT': 'Connecticut', 'DE': 'Delaware', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho',\n",
        "    'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas', 'KY': 'Kentucky',\n",
        "    'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland', 'MA': 'Massachusetts', 'MI': 'Michigan',\n",
        "    'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska',\n",
        "    'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NC': 'North Carolina',\n",
        "    'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania',\n",
        "    'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota', 'TN': 'Tennessee', 'UT': 'Utah',\n",
        "    'VT': 'Vermont', 'VA': 'Virginia', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'\n",
        "}\n",
        "\n",
        "# Add full state names to df_abbrev using the mapping\n",
        "state_loan_sum['geographic area'] = state_loan_sum['BorrowerState'].map(state_mapping)\n",
        "\n",
        "#aggregate\n",
        "merged_df = pd.merge(state_loan_sum, state_population, on='geographic area')\n",
        "\n",
        "#save the cleaned merged data to output folder\n",
        "output_file_path = '/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/Output/CleanMergeData.csv'\n",
        "merged_df.to_csv(output_file_path, index=False)\n",
        "#loan ammount per capita\n",
        "merged_df['percapita'] = merged_df['CurrentApprovalAmount'] / merged_df.iloc[:, 4]\n",
        "\n",
        "#per 100.000\n",
        "merged_df['LoanAmount_per_100k'] = merged_df['percapita'] * 100000\n",
        "\n",
        "\n",
        "\n",
        "###lets create the csv file\n",
        "\n",
        "# create\n",
        "output_df = merged_df[['geographic area','LoanAmount_per_100k']]\n",
        "\n",
        "#save to csv\n",
        "output_file_path = '/Users/lazarus/Library/Mobile Documents/com~apple~CloudDocs/MSc Statistics/Data Engineer/CovidRecovery/CSV/loan_amount_per_100k.csv'\n",
        "output_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "\n"
      ]
    }
  ]
}